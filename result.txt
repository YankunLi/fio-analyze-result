

a) read : io=10240MB, bw=63317KB/s, iops=15829, runt=165607msec
是说执行了10G的IO操作，每秒的速度是63.317MB/s，每秒读写操作次数是15829，运行时长是2分钟45秒
b)slat (usec): min=3, max=335, avg= 9.73, stdev= 5.76
第一个延迟度量是'slat'或提交延迟，意识是说提交这个IO到内核处理用了多久
c)lat (usec): min=44, max=18627, avg=61.33, stdev=17.91
下一个是完成延迟，它是提交到内核和IO完成经过的时间，不包括提交延迟，在fio的旧版本中，它是接近应用级延迟的最好的度量
clat percentiles (usec):
     |  1.00th=[   42],  5.00th=[   45], 10.00th=[   45], 20.00th=[   46],
     | 30.00th=[   47], 40.00th=[   47], 50.00th=[   49], 60.00th=[   51],
     | 70.00th=[   53], 80.00th=[   56], 90.00th=[   60], 95.00th=[   67],
     | 99.00th=[   78], 99.50th=[   81], 99.90th=[   94], 99.95th=[  101],
     | 99.99th=[  112]
我认为'lat'度量是最新的，文档中没有它的介绍，从代码看，好像它是在fio建立IO结构体的时候开始在'clat'之后完成，使得它是展现将要体验应用的
内容最好的一个

完成延迟百分百的是一目了然和可能是最有用的输出。从代码来看它不是 slat+clat，它是跟随自己的结构体

在配置文件里栈是可配置的。在简要输出里，它有20个域如%d=d;%f=d; 这使得解析比它将要展现的更有趣

为了对比，下面是一个运行完全相同负载的7200 rpm SAS盘的同一部分结果：

clat percentiles (usec):
     |  1.00th=[ 3952],  5.00th=[ 5792], 10.00th=[ 7200], 20.00th=[ 8896],
     | 30.00th=[10304], 40.00th=[11456], 50.00th=[12608], 60.00th=[13760],
     | 70.00th=[15168], 80.00th=[16768], 90.00th=[18816], 95.00th=[20608],
     | 99.00th=[23424], 99.50th=[24192], 99.90th=[26752], 99.95th=[28032],
     | 99.99th=[30080]

    bw (KB  /s): min=52536, max=75504, per=67.14%, avg=63316.81, stdev=4057.09

出了per=part外，带宽的内容都是自解释的，文档中说它的意义是位了测试多个负载的单一设备，因此能看到有多少IO是被每个进程消耗的。
当fio是运行在多个设备上时，就像接下来的输出，但是当ssd和旋转设备混合使用时它不提供更多的语意

   bw (KB  /s): min=   71, max=  251, per=0.36%, avg=154.84, stdev=18.29

    lat (usec) :   2= 0.01%,   4=0.01%,  10=0.01%,   20=0.01%, 50=51.41%
    lat (usec) : 100=48.53%, 250=0.06%, 500=0.01%, 1000=0.01%
    lat (msec) :   2= 0.01%,   4=0.01%,  10=0.01%,   20=0.01%

延迟分发部分带给我数个关卡去理解。这是一系列的度量，三行没有使用同样的单位，第三行切换到一个千毫秒级去维护文本宽度控制。读取最后一行就像2000,4000,10000和20000usec

当这是一个延迟分发的时候，它是说51.41%的请求使用的时间少于50usec，48.53%的请求少于100uesc等等
  lat (msec) : 4=1.07%, 10=27.04%, 20=65.43%, 50=6.46%, 100=0.01%
当你用一个快速脚本分析这个输出的时候，如果里边没有数据，你可能想知道延迟部分将要忽略记录和全部的行。例如，曾被我提到的SAS设备不管理任何超过一毫秒的IO，因此它只有一行
cpu          : usr=5.32%, sys=21.95%, ctx=2829095, majf=0, minf=21

接下来是因为上下文切换导致的主要和次要页面失败的用户/系统 CPU使用百分比。因为测试被配置的使用直接IO，因此有很少的页面失败：
IO depths    : 1=100.0%, 2=0.0%, 4=0.0%, 8=0.0%, 16=0.0%, 32=0.0%, &gt;=64=0.0%

fio有一个iodepth设置用来控制在任何时间有多少IO分发给系统。这完全是应用方面的，意味着它和设备的IO队列做不同的事情，iodepth设置为1因此IO深度在100%的时间里一直是一个1

submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, &gt;=64=0.0%
complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, &gt;=64=0.0%

submit 和complete显示在任意时间通过fio提交的IO和在一个时间内完成的数字，在超负荷测试的情况下会产生这个输出，iodepth的默认值是1，因此在1-4桶放置的结果100%的IO被提交位1

当我到处测试各种任务的时候这将变得更多更有意思


issued    : total=r=2621440/w=0/d=0, short=r=0/w=0/d=0
被发布的IO的数量，这里有些怪异因为这是一个50/50 读/写 的负载，因为应该是一个同等数量的写，我怀疑 unified_rw_reporting打开使得fio统计所有的IO作为读

如果在做直接IO测试的时候看到短IO那么可能哪里搞错了，我在linux内核中发现这可能会在EOF和设备可能结尾的地方发生

latency   : target=0, window=0, percentile=100.00%, depth=1

fio能被配置一个延迟目标，这将使得去调整吞吐量制单能连续的命中被配置的延迟目标，我没有弄乱这个，在时间或基于大小的测试中，这一行将一直看起来一样，所有这四个值显示配置设置
latency_target, latency_window, latency_percentile, and iodepth.

Run status group 0 (all jobs):

fio支持未了聚合去分组做不同的测试，例如，我能在一个文件中配置ssd和hdd，但是配置组去分别显示IO，我现在不做这些，但将来配置需要这个功能

MIXED: io=12497MB, aggrb=42653KB/s, minb=277KB/s, maxb=41711KB/s, mint=300000msec, maxt=300012msec

最后，吞吐量和时间的汇总，io= 表明在总计里IO完成的数量，对于时间的测试它是多变的，对于大小的测试可能匹配尺寸参数。aggrg是通过所有的进程/设备得出的合计带宽，
minb/maxb显示遵循的最小/最大带宽。mint/maxt显示所有测试的最短&最长的时间。类似于io= 参数。对于基于时间的测试将匹配runtim参数，在基于大小的测试将变化

因为运行测试打开了 unified_rw_reporting，只能看到MIXED这行，如果关闭了它将分别显示读和写
